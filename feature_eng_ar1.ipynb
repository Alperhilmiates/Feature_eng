import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression

df = pd.read_excel("artec_2.xlsx")
df.head()

df.Growth_Season_Time.count()

X = df.copy()
y = X.pop("total_pest_num")
z= X.pop("Growth_Season_Time")
print(X)
print(y)

X = df.copy()
y = X.pop("total_pest_num")
z= X.pop("Growth_Season_Time")
t= X.pop("Plant_Season")

# Train and score baseline model
baseline = RandomForestRegressor(criterion="mae", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error"
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Baseline Score: {baseline_score:.4}")

X = df.copy()
y = X.pop("total_pest_num")
z= X.pop("Growth_Season_Time")
t= X.pop("Plant_Season")

# Train and score baseline model
baseline = RandomForestRegressor(criterion="absolute_error", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error"
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Baseline Score: {baseline_score:.4}")

Mutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you'd like to use yet. It is:

easy to use and interpret,
computationally efficient,
theoretically well-founded,
resistant to overfitting, and,
able to detect any kind of relationship


import pandas as pd
from sklearn.feature_selection import mutual_info_regression

# Load data
df = pd.read_excel("artec_2.xlsx")
X = df.drop("total_pest_num", axis=1)
y = df["total_pest_num"]

# Convert Date column to integer
X["Growth_Season_Time"] = pd.to_datetime(X["Growth_Season_Time"]).dt.date
X["Growth_Season_Time"] = (X["Growth_Season_Time"] - X["Growth_Season_Time"].min()).dt.days

X_new = X.drop("Plant_Season", axis=1)

# Compute mutual information scores
mi_scores = mutual_info_regression(X_new, y)

# Print scores
for feature, score in zip(X_new.columns, mi_scores):
    print(f"{feature}: {score}")


plt.style.use("seaborn-whitegrid")

df = pd.read_excel("artec_2.xlsx")
df.head()

X = df.copy()
y = X.pop("total_pest_num")

X["Growth_Season_Time"] = pd.to_datetime(X["Growth_Season_Time"])
X["Growth_Season_Time"] = (X["Growth_Season_Time"] - X["Growth_Season_Time"].min()) // pd.Timedelta('1d')

# Label encoding for categoricals
for colname in X.select_dtypes("object"):
    X[colname], _ = X[colname].factorize()
for colname in X.select_dtypes("float"):
    X[colname] = np.round(X[colname]).astype(int)
#     X[colname] = X[colname].astype(np.float32)

# All discrete features should now have integer dtypes (double-check this before using MI!)
discrete_features = X.dtypes == int

X_new = X.drop("Plant_Season", axis=1)

def make_mi_scores(X, y):
    mi_scores = mutual_info_regression(X, y)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X_new, y)
mi_scores

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores: Relationship between Number of Pests and Various Features - Scikit-learn")


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)

sns.relplot(x="Avg_Temperature", y="total_pest_num", data=df)
ax = plt.gca()
ax.set_title("Effect of Avarage Temperature on Pest and Pathogen Control")
ax.set(xlabel='Avarage Temperature', ylabel='Total # of Pests')

sns.lmplot(x="Natural_enemy", y= "total_pest_num" , data=df, hue="Plant_Season")
ax = plt.gca()
ax.set_title("Prediction for Biological Pest and Pathogen Control 2023")
ax.set(xlabel='Biological Pest Control (Natural Enemy)', ylabel='Predicted Total # of Pests')

sns.lmplot(x="pesticide effectiveness %", y="total_pest_num", hue="Plant_Season", data=df)
ax = plt.gca()
ax.set_title("Pesticide Effectiveness % on Pest and Pathogen Control")
ax.set(xlabel='% Effectiveness of Pesticide', ylabel='Total # of Pests')

sns.catplot(x="Plant_Season", y="total_pest_num", data=df, kind="boxen")
ax = plt.gca()
ax.set_title("Effect of Planting Season on Pest and Pathogen Control")
ax.set(xlabel='Plant Season', ylabel='Total # of Pests')


from sklearn.preprocessing import PowerTransformer

# make a copy of the dataset
X = df.copy()
X["Growth_Season_Time"] = pd.to_datetime(X["Growth_Season_Time"])
X["Growth_Season_Time"] = (X["Growth_Season_Time"] - X["Growth_Season_Time"].min()) // pd.Timedelta('1d')

# Label encoding for categoricals
for colname in X.select_dtypes("object"):
    X[colname], _ = X[colname].factorize()

X_transformed = X.copy()

# apply PowerTransformer to all columns except Date
pt = PowerTransformer(method='yeo-johnson', standardize=True)
X_transformed.iloc[:, 1:] = pt.fit_transform(X.iloc[:, 1:])


for colname in X_transformed.select_dtypes("number"):
    X_transformed[colname] = np.log1p(X_transformed[colname])

print(X_transformed)



# Set the width and height of the figure
plt.figure(figsize=(14,7))

# Add title
plt.title("total pest numbers, by feature")

# Heatmap showing average arrival delay for each airline by month
sns.heatmap(data=X_transformed, annot=True)

# Add label for horizontal axis
plt.xlabel("Features")

sns.lmplot(x="Natural_enemy", y="total_pest_num", hue="Plant_Season", data=X_transformed)

sns.lmplot(x="Natural_enemy", y="total_pest_num", hue="Plant_Season", data=df)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression

# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

# Load data
df = pd.read_csv("ames.csv")

# Utility functions from Tutorial
def make_mi_scores(X, y):
    X = X.copy()
    for colname in X.select_dtypes(["object", "category"]):
        X[colname], _ = X[colname].factorize()
    # All discrete features should now have integer dtypes
    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")

features = ["YearBuilt", "MoSold", "ScreenPorch"]
sns.relplot(
    x="value", y="SalePrice", col="variable", data=df.melt(id_vars="SalePrice", value_vars=features), facet_kws=dict(sharex=False),
);

X = df.copy()
y = X.pop('SalePrice')

mi_scores = make_mi_scores(X, y)

print(mi_scores.head(20))
# print(mi_scores.tail(20))  # uncomment to see bottom 20

plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores.head(20))
# plot_mi_scores(mi_scores.tail(20))  # uncomment to see bottom 20

sns.catplot(x="BldgType", y="SalePrice", data=df, kind="boxen");

# YOUR CODE HERE: 
feature = "GrLivArea"
# feature="MoSold"

sns.lmplot(
    x=feature, y="SalePrice", hue="BldgType", col="BldgType",
    data=df, scatter_kws={"edgecolor": 'w'}, col_wrap=3, height=4,
);

feature="MoSold"

sns.lmplot(
    x=feature, y="SalePrice", hue="BldgType", col="BldgType",
    data=df, scatter_kws={"edgecolor": 'w'}, col_wrap=3, height=4,
);

plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

accidents = pd.read_csv("accidents.csv")
autos = pd.read_csv("autos.csv")
concrete = pd.read_csv("concrete.csv")
customer = pd.read_csv("customer.csv")

autos["stroke_ratio"] = autos.stroke / autos.bore

autos[["stroke", "bore", "stroke_ratio"]].head()

autos["displacement"] = (
    np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders
)

autos[["bore","stroke","num_of_cylinders","displacement"]].head()

# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log
accidents["LogWindSpeed"] = accidents.WindSpeed.apply(np.log1p)

# Plot a comparison
fig, axs = plt.subplots(1, 2, figsize=(8, 4))
sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);

roadway_features = ["Amenity", "Bump", "Crossing", "GiveWay",
    "Junction", "NoExit", "Railway", "Roundabout", "Station", "Stop",
    "TrafficCalming", "TrafficSignal"]
accidents["RoadwayFeatures"] = accidents[roadway_features].sum(axis=1)

accidents[roadway_features + ["RoadwayFeatures"]].head(10)

# This will count how many components are in a formulation with the dataframe's 
# built-in greater-than gt method:
components = [ "Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
               "Superplasticizer", "CoarseAggregate", "FineAggregate"]
concrete["Components"] = concrete[components].gt(0).sum(axis=1)

concrete[components + ["Components"]].head(10)

customer[["Type", "Level"]] = (  # Create two new features
    customer["Policy"]           # from the Policy feature
    .str                         # through the string accessor
    .split(" ", expand=True)     # by splitting on " "
                                 # and expanding the result into separate columns
)

customer[["Policy", "Type", "Level"]].head(10)

autos["make_and_style"] = autos["make"] + "_" + autos["body_style"]
autos[["make", "body_style", "make_and_style"]].head()

customer["AverageIncome"] = (
    customer.groupby("State")  # for each state
    ["Income"]                 # select the income
    .transform("mean")         # and compute its mean
)

customer[["State", "Income", "AverageIncome"]].head(10)

customer["StateFreq"] = (
    customer.groupby("State")
    ["State"]
    .transform("count")
    / customer.State.count()
)

customer[["State", "StateFreq"]].head(10)

You could use a transform like this to create a "frequency encoding" for a categorical feature.

If you're using training and validation splits, to preserve their independence, it's best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set's merge method after creating a unique set of values with drop_duplicates on the training set:

# Create splits
df_train = customer.sample(frac=0.5)
df_valid = customer.drop(df_train.index)

# Create the average claim amount by coverage type, on the training set
df_train["AverageClaim"] = df_train.groupby("Coverage")["ClaimAmount"].transform("mean")

# Merge the values into the validation set
df_valid = df_valid.merge(
    df_train[["Coverage", "AverageClaim"]].drop_duplicates(),
    on="Coverage",
    how="left",
)

df_valid[["Coverage", "AverageClaim"]].head(10)

Tips on Creating Features

It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:

Linear models learn sums and differences naturally, but can't learn anything more complex.

Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.

Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.

Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.

Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor


def score_dataset(X, y, model=XGBRegressor()):
    # Label encoding for categoricals
    for colname in X.select_dtypes(["category", "object"]):
        X[colname], _ = X[colname].factorize()
    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)
    score = cross_val_score(
        model, X, y, cv=5, scoring="neg_mean_squared_log_error",
    )
    score = -1 * score.mean()
    score = np.sqrt(score)
    return score


# Prepare data
df = pd.read_csv("ames.csv")
X = df.copy()
y = X.pop("SalePrice")



X_1 = pd.DataFrame()  # dataframe to hold new features

X_1["LivLotRatio"] = df.GrLivArea / df["LotArea"]

X_1["LivLotRatio"]

X_1["Spaciousness"] = df["FirstFlrSF"] + df["SecondFlrSF"] / df.TotRmsAbvGrd

X_1["Spaciousness"] 

X_1["TotalOutsideSF"] = df["WoodDeckSF"] + df["OpenPorchSF"] +df["EnclosedPorch"]+ df["Threeseasonporch"] + df["ScreenPorch"]
X_1["TotalOutsideSF"]

If you've discovered an interaction effect between a numeric feature and a categorical feature, you might want to model it explicitly using a one-hot encoding, like so:

### One-hot encode Categorical feature, adding a column prefix "Cat"
X_new = pd.get_dummies(df.Categorical, prefix="Cat")

### Multiply row-by-row
X_new = X_new.mul(df.Continuous, axis=0)

### Join the new features to the feature set
X = X.join(X_new)

# One-hot encode BldgType. Use `prefix="Bldg"` in `get_dummies`
X_2 = pd.get_dummies(df.BldgType, prefix= "Bldg") 
# Multiply
X_2 = X_2.mul(df.GrLivArea, axis= 0)
X_1 = X_1.join(X_2)
X_1

Let's try creating a feature that describes how many kinds of outdoor areas a dwelling has. Create a feature PorchTypes that counts how many of the following are greater than 0.0:

X_3 = pd.DataFrame()

# YOUR CODE HERE
X_3["PorchTypes"] = df[['WoodDeckSF', 'OpenPorchSF','EnclosedPorch','Threeseasonporch','ScreenPorch']].gt(0.0).sum(axis=1)
X_3

df.MSSubClass.unique()

dene  = df.MSSubClass.str.split('_', 1, expand= True)
dene

X_4 = pd.DataFrame()
X_4['first_name_only'] = df.MSSubClass.str.split('_', n=1)
X_4

X_4['MSClass'] = df.MSSubClass.str.split('_', n=1,expand=True)[0]

X_4

X_5 = pd.DataFrame()

# YOUR CODE HERE
X_5["MedNhbdArea"] = df.groupby(['Neighborhood']).GrLivArea.transform("median")

X_5

X_6 = pd.DataFrame()
X_6["MedNhbdArea"] = df.groupby("Neighborhood")["GrLivArea"].median()
X_6.sort_values(by='MedNhbdArea', ascending=True)

I needed to find the median for a pandas DataFrame and used a piece of code from this previous SO answer: How I do find median using pandas on a dataset?.

I used the following code from that answer:

 data['metric_median'] = data.groupby('Segment')['Metric'].transform('median')
It seemed to work well, so I'm happy about that, but I had a question: how is it that transform method took the argument 'median' without any prior specification? I've been reading the documentation for transform but didn't find any mention of using it to find a median.

Basically, the fact that .transform('median') worked seems like magic to me, and while I have no problem with magic and fancy myself a young Tony Wonder, I'm curious about how it works.


Solution:

I'd recommend diving into the source code to see exactly why this works (and I'm mobile so I'll be terse).

When you pass the argument  'median'  to  tranform  pandas converts this behind the scenes via  getattr  to the appropriate method then behaves like you passed it a function.

Python - Median of pandas dataframe column, I tried df['count'].median() and got the median. But don't know how to proceed further. Can you suggest how I could use pandas/numpy for this. Expected Output : name count distance from median aaaa 2000 ***** I can use any measure as the distance from median (absolute deviation from median, quantiles etc.).

X

# X_new = X.join([X_1, X_2, X_3, X_4, X_5])
X_new = pd.concat([X, X_1, X_2, X_3, X_4, X_5])

X_new

# score_dataset(X_new, y)

